{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_04_NER_and_Intent_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescodisalvo05/polito-deep-nlp/blob/main/Labs/Lab_04_NER_and_Intent_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrHLvIkbUsjZ"
      },
      "source": [
        "#**Deep Natural Language Processing @ PoliTO**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Teaching Assistant:** Moreno La Quatra\n",
        "\n",
        "**Practice 3:** Named Entities Recognition & Intent Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GQde2M-U4KV"
      },
      "source": [
        "## Named Entities Recognition\n",
        "\n",
        "Named-entity recognition (NER) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n",
        "\n",
        "![https://miro.medium.com/max/875/0*mlwDqNm7DFc_4maP.jpeg](https://miro.medium.com/max/875/0*mlwDqNm7DFc_4maP.jpeg)   \n",
        "\n",
        "Text domain is **crucial** while recognizing entities (political, medical, food...)\n",
        "\n",
        "In this practice you will:\n",
        "- Experiment with pre-trained models to extract entities from text\n",
        "- "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VXjktGnWUTm"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9elJruvpWAQ_"
      },
      "source": [
        "### **Question 1: data preparation**\n",
        "\n",
        "The data collection is available [here](https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/NER/wikigold.conll.txt). \n",
        "This dataset was presented in [1][2] and consists of a set of manually annotated Wikipedia text. The data already in [CONLL](https://simpletransformers.ai/docs/ner-data-formats/#text-file-in-conll-format) format. Please read carefully before proceeding with data parsing.\n",
        "\n",
        "You need to extract clean sentences (no annotation) and, for each sentence, text associated to each entity:     \n",
        "- `sentences`: list of sentences\n",
        "- `annotations`: list of list of entities (both string and class information). E.g., `[[('010', 'MISC'), ('Japanese', 'MISC'), ('The Mad Capsule Markets', 'ORG')], [('Osc-Dis', 'MISC'), ('Introduction 010', 'MISC'), ('Come', 'MISC')], ...]`. You can remove I- prefix because the data collection does not actually cotains valuable prefixes.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "[1] Balasuriya, Dominic, et al. \"Named entity recognition in wikipedia.\"\n",
        "    Proceedings of the 2009 Workshop on The People's Web Meets NLP: Collaboratively Constructed Semantic Resources. Association for Computational Linguistics, 2009.\n",
        "\n",
        "[2] Nothman, Joel, et al. \"Learning multilingual named entity recognition\n",
        "    from Wikipedia.\" Artificial Intelligence 194 (2013): 151-175 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeGD-RtlV_0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d78cef-d49c-43fe-9aad-b9eb9081e872"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/NER/wikigold.conll.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-23 17:37:26--  https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/NER/wikigold.conll.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 318530 (311K) [text/plain]\n",
            "Saving to: ‘wikigold.conll.txt’\n",
            "\n",
            "wikigold.conll.txt  100%[===================>] 311.06K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-11-23 17:37:26 (13.9 MB/s) - ‘wikigold.conll.txt’ saved [318530/318530]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK19r9la4jUd"
      },
      "source": [
        "def split_text_label(filename):\n",
        "    f = open(filename)\n",
        "    split_labeled_text = []\n",
        "    sentence = []\n",
        "    for line in f:\n",
        "        if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
        "             if len(sentence) > 0:\n",
        "                 split_labeled_text.append(sentence)\n",
        "                 sentence = []\n",
        "             continue\n",
        "        splits = line.split(' ')\n",
        "        sentence.append([splits[0],splits[-1].rstrip(\"\\n\")])\n",
        "    if len(sentence) > 0:\n",
        "        split_labeled_text.append(sentence)\n",
        "        sentence = []\n",
        "    return split_labeled_text\n",
        "sentences_with_labels = split_text_label(\"wikigold.conll.txt\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmIad5_ZUg6X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a3fed59a-ba51-4830-f8a4-75ee3a6bf14c"
      },
      "source": [
        "# Your code here\n",
        "\n",
        "cleaned_sentences = [' '.join([t[0] for t in sentence]) for sentence in sentences_with_labels]\n",
        "cleaned_sentences[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'010 is the tenth album from Japanese Punk Techno band The Mad Capsule Markets .'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frz3balsdNlB"
      },
      "source": [
        "# we have to take care about the ones that are splitted into\n",
        "# multiple tokens. Therefore we will take care about the \"preceeding\"\n",
        "# one, that by default is the escape \"0\"\n",
        "\n",
        "labels, complete_labels = [], set()\n",
        "\n",
        "for sentence in sentences_with_labels:\n",
        "\n",
        "  current_labels, previous_label = [], \"O\"\n",
        "\n",
        "  # since the given \"entity\" can be composed from \n",
        "  # different words, it must be \"constructed\"\n",
        "  constructed_entity = \"\"\n",
        "\n",
        "  for word, current_label in sentence:\n",
        "\n",
        "    complete_labels.add(current_label)\n",
        "\n",
        "    # we can append the previous one\n",
        "    if  current_label == \"O\" and previous_label != \"O\":\n",
        "      current_labels.append((constructed_entity.strip(), previous_label.split(\"-\")[1])) # remove I-\n",
        "      constructed_entity = \"\" # initialize again\n",
        "\n",
        "    # start a new one\n",
        "    if current_label != \"O\" and previous_label == \"O\":\n",
        "      constructed_entity = word + \" \"\n",
        "\n",
        "\n",
        "    # add element to the same label\n",
        "    if current_label != \"O\" and previous_label == current_label:\n",
        "      constructed_entity = constructed_entity + word + \" \"\n",
        "\n",
        "    # new entity\n",
        "    if current_label != \"O\" and previous_label != \"O\" and previous_label != current_label:\n",
        "      current_labels.append((constructed_entity.strip(), previous_label.split(\"-\")[1])) # remove I-\n",
        "      constructed_entity = word + \" \" # initialize again with the new word\n",
        "\n",
        "    previous_label = current_label\n",
        "\n",
        "  labels.append(current_labels)\n",
        "\n",
        "# remove \"O\" and \"I-\"\n",
        "complete_labels = [l.split(\"-\")[1] for l in list(complete_labels) if l != \"O\"]"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eToaMRZmlKqx",
        "outputId": "5d8d40d2-5019-4961-be2d-983a7842c7c3"
      },
      "source": [
        "labels[:5]"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('010', 'MISC'), ('Japanese', 'MISC'), ('The Mad Capsule Markets', 'ORG')],\n",
              " [('Osc-Dis', 'MISC'), ('Introduction 010', 'MISC'), ('Come', 'MISC')],\n",
              " [('Kojima Minoru', 'PER'),\n",
              "  ('Good Day', 'MISC'),\n",
              "  ('Wardanceis', 'MISC'),\n",
              "  ('UK', 'LOC'),\n",
              "  ('Killing Joke', 'ORG')],\n",
              " [('XXX can of This', 'MISC')],\n",
              " [('Cannabis', 'MISC'),\n",
              "  ('Cannabis', 'MISC'),\n",
              "  ('P.O.P', 'MISC'),\n",
              "  ('HUMANITY', 'MISC')]]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iayw3qup99c",
        "outputId": "13af964e-2ca5-48e0-88ea-fba657e33cd5"
      },
      "source": [
        "complete_labels"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ORG', 'MISC', 'LOC', 'PER']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY3VJcC9Bvnf"
      },
      "source": [
        "### **Question 2: inference with spacy for entity recognition**\n",
        "\n",
        "Spacy models comes with built-in NER models. Instantiate a [spacy model](https://spacy.io/usage/models) for the english language and get, for each sentence in the data collection, its named entities extracted from the model.\n",
        "\n",
        "Given that, the provided data collection only contains a subset of spacy labels map all the classes not available in the data collection to the `MISC` class. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXMAjNvaIoj5"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Dpa0pRYqiAH"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for sentence in cleaned_sentences:\n",
        "\n",
        "  out = nlp(sentence)\n",
        "\n",
        "  entities = []\n",
        "\n",
        "  # https://github.com/explosion/spaCy/issues/1131\n",
        "  # out.ents!\n",
        "  for e in out.ents:\n",
        "    if e.label_ not in complete_labels:\n",
        "      entities.append((e.text, 'MISC'))\n",
        "    else:\n",
        "      entities.append((e.text, e.label_))\n",
        "\n",
        "  predictions.append(entities)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SDDbG-GrTEh",
        "outputId": "51007186-0f01-475d-9a75-b058fe21612d"
      },
      "source": [
        "predictions[:5]"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('010', 'MISC'),\n",
              "  ('tenth', 'MISC'),\n",
              "  ('Japanese', 'MISC'),\n",
              "  ('The Mad Capsule Markets', 'ORG')],\n",
              " [('Osc-Dis', 'MISC'), ('Introduction 010', 'MISC')],\n",
              " [('Kojima Minoru', 'MISC'),\n",
              "  ('Good Day', 'MISC'),\n",
              "  ('Wardanceis', 'MISC'),\n",
              "  ('UK', 'MISC'),\n",
              "  ('Killing Joke', 'MISC')],\n",
              " [('XXX', 'ORG')],\n",
              " [('Cannabis', 'ORG'),\n",
              "  ('Cannabis', 'ORG'),\n",
              "  ('P.O.P', 'ORG'),\n",
              "  ('HUMANITY', 'ORG')]]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf0NzZzjPheL"
      },
      "source": [
        "### **Question 3: compute metrics for evaluating NER**\n",
        "\n",
        "Use [eval4ner](https://github.com/cyk1337/eval4ner) to evaluate the spacy model for NER on the parsed dataset.\n",
        "\n",
        "**Note**: please use `pip install git+https://github.com/MorenoLaQuatra/eval4ner` to use a fixed version of the library. Before passing the parameter to the evaluation function, create a deepcopy of each variable:\n",
        "\n",
        "The issue has been already reported to the original author."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACNK6UU98VZr"
      },
      "source": [
        "! pip install git+https://github.com/MorenoLaQuatra/eval4ner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlJM3PVa8SGS"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC19UMUZdJxc"
      },
      "source": [
        "### **Question 4: inference with transformers pipeline**\n",
        "\n",
        "Transformer-based models can be fine-tuned for token-level classification. The most relevant task in this class is NER. Use [transformers pipelines](https://huggingface.co/transformers/master/main_classes/pipelines.html#transformers.TokenClassificationPipeline) to recognize entities in the previous data collection. \n",
        "\n",
        "Evaluate the model using the same procedure of Q3.\n",
        "\n",
        "**Note:** the output of the pipeline differs with respect to spacy. Please be sure to process data correctly before running evaluation.\n",
        "\n",
        "**Note 2:** `ignore_labels` parameter could be useful to correctly parse entities.\n",
        "\n",
        "**Note 3:** `##` symbol is used when a token is a continuation of a previous one (Poli + ##TO)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6pQuOdTZZVo"
      },
      "source": [
        "%%capture\n",
        "! pip install datasets transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8I-9jTBI_5F"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIrJbaVgi_ja"
      },
      "source": [
        "## Intent Detection\n",
        "\n",
        "In data mining, intention mining or intent mining is the problem of determining a user's intention from logs of his/her behavior in interaction with a computer system, such as in search engines. Intent Detection is the identification and categorization of what a user online intended or wanted to find when they type or speak with a conversational agent (or a search engine).\n",
        "\n",
        "![https://d33wubrfki0l68.cloudfront.net/32e2326762c75a0357ab1ae1976a60d4bbce724b/f4ac0/static/a5878ba6b0e4e77163dc07d07ecf2291/2b6c7/intent-classification-normal.png](https://d33wubrfki0l68.cloudfront.net/32e2326762c75a0357ab1ae1976a60d4bbce724b/f4ac0/static/a5878ba6b0e4e77163dc07d07ecf2291/2b6c7/intent-classification-normal.png)\n",
        "\n",
        "Data source (ATIS dataset): https://github.com/yvchen/JointSLU ; https://www.kaggle.com/siddhadev/atis-dataset-clean/home\n",
        "\n",
        "Use provided train/dev/test split accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L6-2ABir0yS"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/IntentDetection/atis.train.csv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/IntentDetection/atis.dev.csv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P4/IntentDetection/atis.test.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14nJxxjTj8Q9"
      },
      "source": [
        "### **Question 5: two-step classification model**\n",
        "\n",
        "Train a classification model to identify the intent from sentence text. The model should leverage on pretrained BERT model to generate features for each sentence (No-finetuning).\n",
        "\n",
        "![https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/no_finetuning.png?raw=true](https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/no_finetuning.png?raw=true)\n",
        "\n",
        "\n",
        "Assess the performance of the generated model by using the **classification accuracy**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STJSjSovq46n"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1xrNtQTq5WW"
      },
      "source": [
        "### **Question 6: finetuning end-to-end classification model**\n",
        "\n",
        "Train a new BERT model for the task of [sequence classification](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification) (include BERT fine-tuning).  \n",
        "\n",
        "![https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/finetuning.png?raw=true](https://github.com/MorenoLaQuatra/DeepNLP/blob/main/practices/P4/IntentDetection/finetuning.png?raw=true)\n",
        "\n",
        "Assess the performance of the generated model by using the **classification accuracy**.\n",
        "\n",
        "Which model has better performance?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqlKATw5sJAY"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FAAdSip8MOo"
      },
      "source": [
        ""
      ]
    }
  ]
}