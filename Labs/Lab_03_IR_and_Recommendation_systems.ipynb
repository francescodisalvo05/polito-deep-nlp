{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Lab_03_IR_and_Recommendation_systems.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eIamxNpr_E99",
        "P1XxgpAo-VOg",
        "76Ibyq9Mqn3X",
        "ctgQc008zSB8",
        "yohg5-V40aUW"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o7x8zVR9gSK"
      },
      "source": [
        "#**Deep Natural Language Processing @ PoliTO**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Teaching Assistant:** Moreno La Quatra\n",
        "\n",
        "**Practice 3:** Information Retrieval & Elastic Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIamxNpr_E99"
      },
      "source": [
        "### Download and setup ElasticSearch on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SziAua4o9UIl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "484b1157-ba00-433f-c2fb-797585db8294"
      },
      "source": [
        "# Download and extract elasticsearch\n",
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.10.1-linux-x86_64.tar.gz\n",
        "!tar -xzf elasticsearch-7.10.1-linux-x86_64.tar.gz\n",
        "!chown -R daemon:daemon elasticsearch-7.10.1"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-22 18:51:34--  https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.10.1-linux-x86_64.tar.gz\n",
            "Resolving artifacts.elastic.co (artifacts.elastic.co)... 34.120.127.130, 2600:1901:0:1d7::\n",
            "Connecting to artifacts.elastic.co (artifacts.elastic.co)|34.120.127.130|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 318801277 (304M) [application/x-gzip]\n",
            "Saving to: ‘elasticsearch-7.10.1-linux-x86_64.tar.gz.1’\n",
            "\n",
            "elasticsearch-7.10. 100%[===================>] 304.03M  41.7MB/s    in 7.7s    \n",
            "\n",
            "2021-11-22 18:51:42 (39.7 MB/s) - ‘elasticsearch-7.10.1-linux-x86_64.tar.gz.1’ saved [318801277/318801277]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN1ll47sQIq-"
      },
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "\n",
        "# If issues are encountered with this section, ES can be manually started as follows:\n",
        "# ./elasticsearch-7.10.1/bin/elasticsearch\n",
        "\n",
        "# Start and wait for server\n",
        "server = Popen(['elasticsearch-7.10.1/bin/elasticsearch'], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1))\n",
        "!sleep 30"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAAzxrUQFg_g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64979edd-88a4-424d-9688-93733c9d0be1"
      },
      "source": [
        "# wait a bit then test\n",
        "!curl -X GET \"localhost:9200/\""
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"name\" : \"8823589fd85e\",\n",
            "  \"cluster_name\" : \"elasticsearch\",\n",
            "  \"cluster_uuid\" : \"3f7DbuAgTX-pWli9Vs43Iw\",\n",
            "  \"version\" : {\n",
            "    \"number\" : \"7.10.1\",\n",
            "    \"build_flavor\" : \"default\",\n",
            "    \"build_type\" : \"tar\",\n",
            "    \"build_hash\" : \"1c34507e66d7db1211f66f3513706fdf548736aa\",\n",
            "    \"build_date\" : \"2020-12-05T01:00:33.671820Z\",\n",
            "    \"build_snapshot\" : false,\n",
            "    \"lucene_version\" : \"8.7.0\",\n",
            "    \"minimum_wire_compatibility_version\" : \"6.8.0\",\n",
            "    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n",
            "  },\n",
            "  \"tagline\" : \"You Know, for Search\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2bb-wW4sZuw"
      },
      "source": [
        "## Information Retrieval\n",
        "\n",
        "Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of **texts**, images or sounds. (source: Wikipedia).\n",
        "\n",
        "This practice is intended for the creation of a wikipedia-based search engine. For the purpose of the practice, only a subset of the wikipedia pages will be used.\n",
        "\n",
        "Data Source: https://snap.stanford.edu/data/wikispeedia.html "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1XxgpAo-VOg"
      },
      "source": [
        "### **Question 1: Pagerank scores**\n",
        "Exploiting the wikipedia citation network, compute, for each page, its associated [pagerank](http://ilpubs.stanford.edu:8090/422/) score.\n",
        "\n",
        "What is the page with the highest Pagerank score?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkbxbh25sZNu"
      },
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P3/wikipedia_network/articles.tsv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P3/wikipedia_network/categories.tsv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P3/wikipedia_network/links.tsv"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyU76QUUqGI_"
      },
      "source": [
        "%%capture\n",
        "! pip install elasticsearch==7.10.1\n",
        "! pip install networkx"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK7a3wt3GvIN"
      },
      "source": [
        "## comments are on my own\n",
        "\n",
        "from urllib.parse import unquote\n",
        "\n",
        "list_articles = open(\"articles.tsv\").read() # full text of articles separated by \\n\n",
        "list_articles = list_articles.split(\"\\n\") # get list of the articles\n",
        "list_articles = [l for l in list_articles if l!= \"\"] # remove the \"blank\" ones\n",
        "list_articles = [l for l in list_articles if l[0] != \"#\"] # remove the ones with \"#\"\n",
        "unquoted_list_articles = [unquote(l) for l in list_articles if l[0] != \"#\"] # Replace %xx escapes with their single-character equivalent. \n",
        "\n",
        "dict_articles = {}\n",
        "for i, l in enumerate(unquoted_list_articles):\n",
        "    dict_articles[l] = {}\n",
        "    dict_articles[l][\"ID\"] = l # unquoted_list_articles\n",
        "    dict_articles[l][\"quoted_ID\"] = list_articles[i] # quoted article"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZWrp69VGvF4"
      },
      "source": [
        "# same as before\n",
        "\n",
        "from urllib.parse import unquote\n",
        "\n",
        "list_categories = open(\"categories.tsv\").read()\n",
        "list_categories = list_categories.split(\"\\n\")\n",
        "list_categories = [l for l in list_categories if l!= \"\"]\n",
        "list_categories = [l for l in list_categories if l[0] != \"#\"]\n",
        "\n",
        "for l in list_categories:\n",
        "    k, v = l.split(\"\\t\")\n",
        "    k = unquote(k)\n",
        "    v = unquote(v)\n",
        "    if \"categories\" in dict_articles[k].keys():\n",
        "        dict_articles[k][\"categories\"].append(v)\n",
        "    else:\n",
        "        dict_articles[k][\"categories\"] = [v]\n",
        "    \n",
        "# print (dict_articles)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZGS5zv5GvDo"
      },
      "source": [
        "# same as before\n",
        "\n",
        "from urllib.parse import unquote\n",
        "\n",
        "list_links = open(\"links.tsv\").read()\n",
        "list_links = list_links.split(\"\\n\")\n",
        "list_links = [l for l in list_links if l!= \"\"]\n",
        "list_links = [l for l in list_links if l[0] != \"#\"]\n",
        "\n",
        "for l in list_links:\n",
        "    s, t = l.split(\"\\t\")\n",
        "    s = unquote(s)\n",
        "    t = unquote(t)\n",
        "    if \"out_links\" in dict_articles[s].keys(): # add a new out link\n",
        "        dict_articles[s][\"out_links\"].append(t) \n",
        "    else:\n",
        "        dict_articles[s][\"out_links\"] = [t]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yV7vO3aGvA5"
      },
      "source": [
        "# print (dict_articles[\"Áedán_mac_Gabráin\"])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU03LqvbGu-d"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ski1dspRCWeg"
      },
      "source": [
        "import networkx as nx\n",
        "from networkx.algorithms.link_analysis.pagerank_alg import pagerank\n",
        "\n",
        "# first, we have to create the edges and the nodes\n",
        "edges = []\n",
        "nodes = []\n",
        "\n",
        "# iterate over all the items in the dictionary\n",
        "# node = k\n",
        "# edges = (k,link) for link in v[\"out_links\"]\n",
        "for k, v in dict_articles.items():\n",
        "  nodes.append(k)\n",
        "  \n",
        "  # not all the articles has the \"out_links\" key\n",
        "  if \"out_links\" in v.keys():\n",
        "    for link in v[\"out_links\"]:\n",
        "      edges.append((k,link))\n",
        "\n",
        "# create a directed graph\n",
        "network = nx.DiGraph()\n",
        "\n",
        "# define nodes and edges on the network\n",
        "network.add_nodes_from(nodes)\n",
        "network.add_edges_from(edges)\n",
        "\n",
        "p = nx.pagerank(network, max_iter=100)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ9H5156EZ1i",
        "outputId": "fa099688-f25f-4590-d5b9-aeadd7078134"
      },
      "source": [
        "# sort in descending order\n",
        "ordered_scores = sorted(list(p.items()), key=lambda x : -x[1])\n",
        "ordered_scores[:10]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('United_States', 0.00956180652731311),\n",
              " ('France', 0.0064200413810133585),\n",
              " ('Europe', 0.006337014005458885),\n",
              " ('United_Kingdom', 0.006232394913963077),\n",
              " ('English_language', 0.004862980440047761),\n",
              " ('Germany', 0.00482224267836269),\n",
              " ('World_War_II', 0.0047226367934437305),\n",
              " ('England', 0.0044723357530703466),\n",
              " ('Latin', 0.004422148441338466),\n",
              " ('India', 0.004033922521194668)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76Ibyq9Mqn3X"
      },
      "source": [
        "### **Question 2: Wikipedia pages indexing**\n",
        "\n",
        "Create a new index in ElasticSearch and Index the Wikipedia webpage (alongiside with their content). The content of each page can be found at `plaintext_articles/QUOTED_ID_OF_THE_DOC.txt`\n",
        "\n",
        "NB: pagerank score must be a field of the indexed doc\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8Z9I-eWu1KA"
      },
      "source": [
        "%%capture\n",
        "! wget https://github.com/MorenoLaQuatra/DeepNLP/raw/main/practices/P3/plaintext_articles.zip\n",
        "! unzip plaintext_articles.zip"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay4m585sD9Nl"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZypHatkeezUa"
      },
      "source": [
        "# es.indices.delete(index='wikipedia', ignore=[400, 404])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUjUwIDyHETr"
      },
      "source": [
        "from elasticsearch import Elasticsearch\n",
        "\n",
        "es = Elasticsearch()\n",
        "res = es.indices.create(index='wikipedia', ignore=400)\n",
        "\n",
        "\n",
        "for k, v in dict_articles.items():\n",
        "  \n",
        "  text = open(\"plaintext_articles/\" + v[\"quoted_ID\"] + \".txt\").read()\n",
        "  v[\"text\"] = text.replace(\"\\n\", \" \")\n",
        "  v[\"pagerank\"] = p[k]\n",
        "\n",
        "\n",
        "# index all the updated elements\n",
        "for k,v in dict_articles.items():\n",
        "  res = es.index(index=\"wikipedia\", id=v[\"ID\"], body = v)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwKPlTnAWb3d"
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "def print_res(res):\n",
        "  print(f\"# Results = {res['hits']['total']['value']}\")\n",
        "\n",
        "  x = PrettyTable()\n",
        "  x.field_names = [\"ID\", \"Score\", \"Pagerank\"] \n",
        "\n",
        "  for r in res[\"hits\"][\"hits\"]:\n",
        "      x.add_row([r[\"_source\"][\"ID\"], r[\"_score\"], r[\"_source\"][\"pagerank\"]])\n",
        "\n",
        "  print(x)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctgQc008zSB8"
      },
      "source": [
        "### **Question 3: Querying ElasticSearch**\n",
        "\n",
        "Perform a query using ElasticSearch. Look for your favorite content (choose and report 3 of them) on the full text of the articles.\n",
        "\n",
        "E.g.:\n",
        "- query 1 : \"The capital of Italy\" (surprised by the result?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "airbSz1qzuAu"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1ZjQVy5S9Cw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8103e8aa-6c44-42fd-8ea3-17ce8f7e677c"
      },
      "source": [
        "query_body = {\n",
        "    \"query\" : {\n",
        "          \"match\" : { \"text\" : \"The capital of Italy\"}\n",
        "    }\n",
        "}\n",
        "\n",
        "res = es.search(index='wikipedia', body=query_body)\n",
        "print_res(res)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Results = 4604\n",
            "+------------------+-----------+------------------------+\n",
            "|        ID        |   Score   |        Pagerank        |\n",
            "+------------------+-----------+------------------------+\n",
            "|      Turin       | 6.4071617 | 0.0001972710679501955  |\n",
            "|       Rome       |  6.325004 | 0.0016184558611025222  |\n",
            "|    San_Marino    | 6.1934733 | 0.0002833258882520379  |\n",
            "|      Harare      | 6.0852537 | 6.195756206059539e-05  |\n",
            "|   Ouagadougou    |  6.076568 | 7.352589008326316e-05  |\n",
            "|     Sarajevo     | 6.0530405 | 0.00017634954023473427 |\n",
            "|      Milan       |  5.998312 | 0.0005573211244109519  |\n",
            "|     Croatia      |  5.932445 | 0.0006357112448291552  |\n",
            "| Byzantine_Empire | 5.9303164 | 0.0012913446974865692  |\n",
            "|    Montenegro    |  5.907314 |  0.000441054799573285  |\n",
            "+------------------+-----------+------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yohg5-V40aUW"
      },
      "source": [
        "### **Question 4: integrating pagerank scores**\n",
        "\n",
        "Create a template query to include pagerank while computing the score (`_score`). \n",
        "\n",
        "Use the [Script score](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#function-script-score) to generate an hybrid score (`_score + pagerank_score * 250`). \n",
        "\n",
        "Perform the same set of queries with this modification, does it change the results?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3kg790tEZn_"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lljEq7WGbjAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b9b1211-9982-445c-9a5c-490678e6c825"
      },
      "source": [
        "query_body = {\n",
        "    \"query\" : {\n",
        "        \"function_score\" : {\n",
        "            \"query\" : {\n",
        "                \"match\" : {\"text\" : \"The capital of Italy\"}\n",
        "            },\n",
        "            \"script_score\" : {\n",
        "                \"script\" : {\n",
        "                  \"source\" : \"if (doc.containsKey('pagerank')) {return _score + 250.0 * doc['pagerank'].value} else {return _score}\"\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "res = es.search(index='wikipedia', body=query_body)\n",
        "print_res(res)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Results = 4604\n",
            "+------------------+-----------+------------------------+\n",
            "|        ID        |   Score   |        Pagerank        |\n",
            "+------------------+-----------+------------------------+\n",
            "|       Rome       |  42.56486 | 0.0016184558611025222  |\n",
            "|      Turin       | 41.367706 | 0.0001972710679501955  |\n",
            "|    San_Marino    | 38.797806 | 0.0002833258882520379  |\n",
            "|      Harare      |  37.12457 | 6.195756206059539e-05  |\n",
            "| Byzantine_Empire | 37.083176 | 0.0012913446974865692  |\n",
            "|   Ouagadougou    | 37.036377 | 7.352589008326316e-05  |\n",
            "|     Sarajevo     | 36.906162 | 0.00017634954023473427 |\n",
            "|      Milan       | 36.815495 | 0.0005573211244109519  |\n",
            "|     Croatia      | 36.136734 | 0.0006357112448291552  |\n",
            "|    Montenegro    | 35.547718 |  0.000441054799573285  |\n",
            "+------------------+-----------+------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lQzbdvvelCS"
      },
      "source": [
        "### **Question 5: integrate semantic dense-vectors**\n",
        "\n",
        "Generate a new index (\"wiki-semantic-search\") including all the information of the previous one plus an additional field that contains a BERT-based embedding vector of the `full_text` of the article. Once indexing is completed, repeat the same queries for a qualitative evaluation of the IR system. \n",
        "\n",
        "**Some hints below:**\n",
        "- Use Sentence-BERT pretrained encoders (www.sbert.net). Choose the most suitable pretrained model (trade off between speed and accuracy). E.g., `multi-qa-MiniLM-L6-cos-v1`\n",
        "- Use cosine similarity to compute the similarity between queries and full text of the article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBwJIcPD0YQf"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMfjagoifrcY"
      },
      "source": [
        "%%capture\n",
        "!pip install sentence-transformers\n",
        "!pip3 install Cython"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcJulq50zfwL"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "sbert = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c721HwNMtQag"
      },
      "source": [
        "# we need a list of sentences, therefore we have to\n",
        "# iterate the first time over the dictionary and\n",
        "# get all the texts\n",
        "\n",
        "full_text_list = [ v[\"text\"] for k, v in dict_articles.items()]\n",
        "\n",
        "encodings = sbert.encode(full_text_list)\n",
        "\n",
        "for idx, (k, v) in enumerate(dict_articles.items()):\n",
        "  v[\"encoding\"] = encodings[idx].tolist()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF3KUfKCC8aV"
      },
      "source": [
        "# create mapping\n",
        "\n",
        "dense_dim = len(encodings[0])\n",
        "\n",
        "index_properties = {}\n",
        "index_properties['settings']={ \"number_of_shards\": 2, \"number_of_replicas\": 1}\n",
        "index_properties['mappings']={ \"dynamic\": \"true\", \"_source\": { \"enabled\": \"true\" }, \"properties\": {}}\n",
        "for t in ['ID', 'quoted_ID', 'text']: \n",
        "    index_properties['mappings']['properties'][t]={ \"type\": \"text\" }\n",
        "for t in ['pagerank']: \n",
        "    index_properties['mappings']['properties'][t]={ \"type\": \"float\" }\n",
        "for d in [\"encoding\"]: \n",
        "    index_properties['mappings']['properties'][d]={ \"type\": \"dense_vector\", \"dims\": dense_dim }"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke2xcjWNFjIc"
      },
      "source": [
        "from elasticsearch import Elasticsearch\n",
        "\n",
        "es = Elasticsearch()\n",
        "es.indices.create(index=\"wiki-semantic-search\", body=index_properties)\n",
        "\n",
        "# upload the updated values\n",
        "for k,v in dict_articles.items():\n",
        "\n",
        "  # we have to follow the index structure\n",
        "  curr_body = {\n",
        "      \"ID\" : v[\"ID\"],\n",
        "      \"quoted_ID\" : v[\"quoted_ID\"],\n",
        "      \"text\" : v[\"text\"],\n",
        "      \"pagerank\" : v[\"pagerank\"],\n",
        "      \"encoding\" : v[\"encoding\"]\n",
        "  }\n",
        "\n",
        "  res = es.index(index=\"wikipedia-semantic-search\", id=v[\"ID\"], body = curr_body)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTEFw41y06lN"
      },
      "source": [
        "query_body = {\n",
        "    \"query\" : {\n",
        "        \"function_score\" : {\n",
        "            \"query\" : {\n",
        "                \"match\" : {\"text\" : \"The capital of Italy\"}\n",
        "            },\n",
        "            \"script_score\" : {\n",
        "                \"script\" : {\n",
        "                  \"params\" : {\"encoding\" : sbert.encode(\"The capital of Italy\")},\n",
        "                  \"source\" : \"cosineSimilarity(params.encoding, doc['encoding']) + 1.0\" # +1 in order to avoid negative values\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "res = es.search(index=\"wikipedia-semantic-search\", body=query_body)\n",
        "\n",
        "# the code is the same as the instructor, but it doesn't work\n",
        "# to do : inspect the definition of the index and the dictionaries"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWHgLZhmszNc"
      },
      "source": [
        "## Content-based Recommender Systems\n",
        "\n",
        "A recommender system is a subclass of information filtering system that seeks to predict the \"rating\" or \"preference\" a user would give to an item. (source: [Wikipedia](https://en.wikipedia.org/wiki/Recommender_system))\n",
        "\n",
        "In this part of the practice you will be required to generate a text-based unsupervised recommendation system (only **content**-based). The final goal is similar to a IR search engine, the main difference relies on **how you define the \"queries\".**\n",
        "\n",
        "The tools at your disposal are:\n",
        "1. `Sentence-BERT model`: should be used to obtain a vector representation of the input data.\n",
        "2. `ElasticSearch`: can be used for indexing movie information and to perform **fast** similarity search.\n",
        "\n",
        "For the recommendation system you need the following information:\n",
        "- Movie's title\n",
        "- Movie's plot\n",
        "- Plot's embedding vector\n",
        "\n",
        "The dataset used for this goal is: [Wikipedia Movie Plots](https://www.kaggle.com/jrobischon/wikipedia-movie-plots). For this practice you will use a truncated version of the data collection to reduce runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfY5cT58aIk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "324be6b6-e983-4f56-a768-a8df4181f4fc"
      },
      "source": [
        "! wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P3/wiki_plots_2005onward.csv\n",
        "import pandas as pd\n",
        "df_movies = pd.read_csv(\"wiki_plots_2005onward.csv\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-22 18:24:23--  https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P3/wiki_plots_2005onward.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45936814 (44M) [text/plain]\n",
            "Saving to: ‘wiki_plots_2005onward.csv’\n",
            "\n",
            "wiki_plots_2005onwa 100%[===================>]  43.81M   135MB/s    in 0.3s    \n",
            "\n",
            "2021-11-22 18:24:24 (135 MB/s) - ‘wiki_plots_2005onward.csv’ saved [45936814/45936814]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyP6CMVMa5Gy"
      },
      "source": [
        "### **Question 6: movie encodings**\n",
        "\n",
        "Use Sentence-BERT model to encode movie plots into fixed-size vectors.\n",
        "\n",
        "NB: the vector dimension is dependent on the choice of the pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1ldnOtob0LC"
      },
      "source": [
        "! pip install sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8zuhSRfFn29"
      },
      "source": [
        "# Your code here\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "sbert = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
        "\n",
        "plot_list = list(df_movies.Plot)\n",
        "title_list = list(df_movies.Title)\n",
        "\n",
        "embedded_plots = sbert.encode(plot_list)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LODAdsacB9m"
      },
      "source": [
        "### **Question 7: ElasticSearch indexing**\n",
        "\n",
        "Create a new ElasticSearch index (`recsys-movies`) and index all movies with their embedding vectors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxJkyzioFs_R"
      },
      "source": [
        "from elasticsearch import Elasticsearch\n",
        "\n",
        "dense_dim = len(embedded_plots[0])\n",
        "\n",
        "index_properties = {}\n",
        "index_properties['settings']={ \"number_of_shards\": 2, \"number_of_replicas\": 1}\n",
        "index_properties['mappings']={ \"dynamic\": \"true\", \"_source\": { \"enabled\": \"true\" }, \"properties\": {}}\n",
        "for t in ['title', 'plot']: \n",
        "    index_properties['mappings']['properties'][t]={ \"type\": \"text\" }\n",
        "for d in [\"embedding\"]: \n",
        "    index_properties['mappings']['properties'][d]={ \"type\": \"dense_vector\", \"dims\": dense_dim }\n",
        "\n",
        "es.indices.delete(index=\"recsys-movies\", ignore=[404])\n",
        "es.indices.create(index=\"recsys-movies\", body=index_properties)\n",
        "\n",
        "\n",
        "# Your code here\n",
        "\n",
        "for i in range(len(plot_list)):\n",
        "\n",
        "  curr_dic = {\n",
        "      \"title\" : title_list[i],\n",
        "      \"plot\" : plot_list[i],\n",
        "      \"embedding\" : embedded_plots[i]\n",
        "  }\n",
        "\n",
        "  res = es.index(index=\"recsys-movies\", body = curr_dic)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oXh5njnftg2"
      },
      "source": [
        "### **Question 8: Query generation**\n",
        "\n",
        "Create a function that accept the following arguments:\n",
        "1. `embedding_model`: Sentence-BERT model used to generate embeddings\n",
        "2. `df_movies`: the dataframe containing all the movies' information\n",
        "3. `movie_title`: a string containing the title of the movie the user is currently watching.\n",
        "\n",
        "It should return the embedding vector associated to the query by looking for the `movie_title` plot in `df_movies`. It uses `embedding_model` to encode it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJj22hTTGc_y"
      },
      "source": [
        "# Your code here\n",
        "\n",
        "def custom_function(embedding_model, df_movies, movie_title):\n",
        "\n",
        "  plot = df_movies[df_movies[\"Title\"] == movie_title].Plot.values\n",
        "\n",
        "  return embedding_model.encode(plot)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgFoTpcZlfkt"
      },
      "source": [
        "### **Question 8: Qualitative evaluation (your personal movie recommendation system)**\n",
        "\n",
        "Evaluate your personal recommendation system by querying for some movies in the data collection. You need to create an elasticsearch query to use the recommendation system (see Q. 5 of this practice).\n",
        "\n",
        "Just some examples:\n",
        "1. title: Harry Potter and the Goblet of Fire\n",
        "2. title: Avengers: Age of Ultron\n",
        "3. title: Star Wars: The Last Jedi\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jywjnPb8Gfb-"
      },
      "source": [
        " # Your code here\n",
        "\n",
        " title = \"Star Wars: The Last Jedi\"\n",
        " title_encoded = custom_function(sbert, df_movies, title)\n",
        "\n",
        " query_body : dict = {\n",
        "    \"query\" : {\n",
        "        \"function_score\" : {\n",
        "            \"query\" : {\n",
        "                \"match_all\" : {}\n",
        "            },\n",
        "            \"script_score\" : {\n",
        "                \"script\" : {\n",
        "                  \"params\" : {\"encoding\" : title_encoded},\n",
        "                  \"source\" : \"cosineSimilarity(params.encoding, doc['encoding']) + 1.0\" # +1 in order to avoid negative values\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "res = es.search(index=\"recsys-movies\", body=query_body)\n",
        "\n",
        "# same as exercise 5.."
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a11nD3sRuv7K"
      },
      "source": [
        "### **Question 9 (Bonus)**\n",
        "\n",
        "Rewrite the function at Q.7 to take multiple movie titles (list of strings). Compute the average vector and use it to obtain recommendations. Perform a qualitative evaluation in this specific case (it is possible to choose movie's titles from the previous list)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFWdTm3zHnkA"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}