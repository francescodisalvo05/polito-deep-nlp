{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Lab_06__Machine_Translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescodisalvo05/polito-deep-nlp/blob/main/Labs/Lab_06__Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vOEr39Bst_5"
      },
      "source": [
        "#**Deep Natural Language Processing @ PoliTO**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Teaching Assistant:** Moreno La Quatra\n",
        "\n",
        "**Practice 6:** Machine Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9F11ek-0L8g"
      },
      "source": [
        "## **Machine Translation**\n",
        "\n",
        "It is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another.\n",
        "\n",
        "![](https://www.deepl.com/img/press/desktop_ENIT_2020-01.png)\n",
        "\n",
        "In this practice you will use data collections provided by [tatoeba](https://tatoeba.org/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41mtAy092sCC"
      },
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P6/train_it_en.tsv\n",
        "!wget https://raw.githubusercontent.com/MorenoLaQuatra/DeepNLP/main/practices/P6/test_it_en.tsv"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsCTUyoq2rZo"
      },
      "source": [
        "### **Question 1: data reading**\n",
        "\n",
        "Read the data collection and store it into your preferred data structure. It will be used in the subsequent steps. \n",
        "\n",
        "Store train and test set into separate data objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rCk2Jcm4eHF"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni6-92OCmK5v"
      },
      "source": [
        "# read train data\n",
        "train_it, train_en = [], [] \n",
        "with open('train_it_en.tsv','r') as train_file:\n",
        "  data = train_file.readlines()\n",
        "  # skip header\n",
        "  for line in data[1:]:\n",
        "\n",
        "    # there are two rows with 7 unpacked elements\n",
        "    # they probably contain a tab on the text\n",
        "    # - skip them\n",
        "\n",
        "    if len(line.split(\"\\t\")) == 5:\n",
        "      _, _, curr_it, _, curr_en = line.split(\"\\t\")\n",
        "      train_it.append(curr_it)\n",
        "      train_en.append(curr_en.strip())"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVD1Ddk4pqCl"
      },
      "source": [
        "# read train data\n",
        "test_it, test_en = [], [] \n",
        "with open('test_it_en.tsv','r') as train_file:\n",
        "  data = train_file.readlines()\n",
        "  # skip header\n",
        "  for line in data[1:]:\n",
        "\n",
        "    # there are two rows with 7 unpacked elements\n",
        "    # they probably contain a tab on the text\n",
        "    # - skip them\n",
        "\n",
        "    if len(line.split(\"\\t\")) == 5:\n",
        "      _, _, curr_it, _, curr_en = line.split(\"\\t\")\n",
        "      test_it.append(curr_it)\n",
        "      test_en.append(curr_en.strip()) # remove final \\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUd9Dle0nTvB",
        "outputId": "ec1f3dd4-769d-423c-98d4-1b08e5d2d4da"
      },
      "source": [
        "train_it[0], train_en[0]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Li aiuteremo domani.', \"We'll help them tomorrow.\")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l36tQ-QqZzq",
        "outputId": "aac377a0-7b09-492e-9cb1-03653eefa3e3"
      },
      "source": [
        "test_it[0], test_en[0]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Non è mai capitato.', 'It never happened.')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyW-AESi3yYe"
      },
      "source": [
        "### **Question 2: pretrained MT models**\n",
        "\n",
        "[EasyNMT](https://github.com/UKPLab/EasyNMT) provides a simple wrapper over HuggingFace transformers library for machine translation. Translate all test sentences from english to italian and viceversa. Store translation in both directions.\n",
        "\n",
        "Note: the choice for the MT model is up to you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA-f6Huz4bWF"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2nFV3akoWk7",
        "outputId": "ed982b85-6834-4296-b262-da1b8bf763a5"
      },
      "source": [
        "!pip install -U easynmt"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting easynmt\n",
            "  Downloading EasyNMT-2.0.1.tar.gz (14 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from easynmt) (4.62.3)\n",
            "Collecting transformers<5,>=4.4\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from easynmt) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from easynmt) (1.19.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from easynmt) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 46.3 MB/s \n",
            "\u001b[?25hCollecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->easynmt) (3.10.0.2)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 37.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (4.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 33.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 33.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (2019.12.20)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 454 kB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=4.4->easynmt) (3.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5,>=4.4->easynmt) (3.0.6)\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.8.1-py2.py3-none-any.whl (208 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext->easynmt) (57.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5,>=4.4->easynmt) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->easynmt) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=4.4->easynmt) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=4.4->easynmt) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=4.4->easynmt) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=4.4->easynmt) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5,>=4.4->easynmt) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5,>=4.4->easynmt) (1.1.0)\n",
            "Building wheels for collected packages: easynmt, fasttext\n",
            "  Building wheel for easynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for easynmt: filename=EasyNMT-2.0.1-py3-none-any.whl size=15446 sha256=4e955f70aadd017d217438a25a99dc2472e3f7a9950b9f88c09ea0f304f58206\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/42/fb/b7711d3296456d5f74e6e265dbdb0e3142158f1bb50382caef\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3121303 sha256=c156e2b2a27f5b02776dd243eb2a3651c43594b4c0a0d2d3757de9bf242fefff\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built easynmt fasttext\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, pybind11, huggingface-hub, transformers, sentencepiece, fasttext, easynmt\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed easynmt-2.0.1 fasttext-0.9.2 huggingface-hub-0.2.1 pybind11-2.8.1 pyyaml-6.0 sacremoses-0.0.46 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.12.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv-4koeH6yvt"
      },
      "source": [
        "### **Question 3: BLEU scores**\n",
        "\n",
        "Evaluate the selected MT model using [BLEU evaluation metric](https://github.com/mjpost/sacrebleu). Report scores for both translation directions (`EN->IT`, `IT->EN`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9dLMLRw7SDu"
      },
      "source": [
        "!pip install sacrebleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BBY0_1X7WAr"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRoH_9ox03ME"
      },
      "source": [
        "### **Question 4: finetuning Seq2Seq model (IT->EN)**\n",
        "\n",
        "Exploit the [Trainer API](https://huggingface.co/transformers/training.html#fine-tuning-in-pytorch-with-the-trainer-api) to finetune and evaluate a [MarianMT](https://arxiv.org/pdf/1804.00344.pdf) sequence to sequence model for machine translation. The documentation for MarianMT is available [here](https://huggingface.co/transformers/model_doc/marian.html).\n",
        "\n",
        "**Note 1:** select the pre-trained model according to the input-output pair (it-en)\n",
        "\n",
        "**Note 2:** for the lab practice, please use a sub-set of the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0V98Y1uTrzR"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwS-7GrlSrxF"
      },
      "source": [
        "### **Question 5: Model evaluation**\n",
        "\n",
        "Evaluate the fine-tuned model on the test set provided with the practice. Compute and report the bleu score for the translation model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL0kUCsVS7b6"
      },
      "source": [
        "# Your code here)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rig_4YXVXIe"
      },
      "source": [
        "### **Question 6: Seq2Seq model implementation (IT->EN) [BONUS]**\n",
        "\n",
        "Implement a lightweight model for machine translation. It must be trainable on the train set of tatoeba available for the practice.\n",
        "\n",
        "**NOTICE:** the goal is to create **your own network**, not to finetune an existing one. You can also leverage LSTM layers instead of transformers.\n",
        "\n",
        "**Note 1:** The choice of the framework (e.g., Keras, Tensorflow, PyTorch) is up to you.\n",
        "\n",
        "**Note 2:** You must write the architecture and training/evaluation procedures (please do not use out-of-the-box HF models)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QPSwu6iTwps"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}